{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVmsGWUFd-tP"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VMJ4av3_YGjM",
        "outputId": "8a01cd7c-9fb3-46ca-9b8d-e7942d6c9aa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''\n",
        "#This was causing some error so i commented it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUT0FSfTbvoc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install pyprind\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3APxWObeA_c"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "* Clone the notebook to your drive.\n",
        "* The notebook has to be submitted in the form of a link giving us **view access**. Share this link in your application.\n",
        "\n",
        "* If you still have any queries, you can reach out to the [core team](https://www.notion.so/Club-Contacts-70a4823e0ae34f35a0aa5d479e449915)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ojuzzfedsb"
      },
      "source": [
        "# Common Technical Questionnaire\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFnXRXAWVSkZ"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Supervised learning is a type of machine learning where the inputs and outputs are mapped through\n",
        "a family of equations, the machine learning model essentially picks the right curve to fit the data.\n",
        "Quantile Regression is a type of supervised learning technique used in statistics and economics. One\n",
        "advantage of quantile regression relative to ordinary least squares regression is that the quantile\n",
        "regression estimates are more robust against outliers in the response measurements.\n",
        "\n",
        "QuantileLossτ (y, ˆy) =\n",
        "{\n",
        "\n",
        "                           τ · (y − ˆy) if y > ˆy\n",
        "\n",
        "                          (1 − τ ) · (ˆy − y) if y ≤ ˆy\n",
        "}\n",
        "\n",
        "where τ is Quantile whose value lies between 0 and 1.\n",
        "Please use this template provided and make changes accordingly for this question alone.\n",
        "Implement a simple Neural Network consisting of 4 nodes, one hidden layer consisting of 5 nodes\n",
        "and output layer consisting of two nodes. Perform quantile regression on the model and observe\n",
        "the loss.\n",
        "**Bonus: Play around with the value of τ to find what value achieves convergence quicker.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bKBykeUZtMq"
      },
      "source": [
        "An example implementation of a simple manual neural network is provided. You may use this as inspiration to complete the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Parameters and the Variables\n",
        "# y = a*x + b\n",
        "import torch\n",
        "x = torch.rand((2, 4), requires_grad=False)\n",
        "y = torch.rand((2, 2), requires_grad=False)\n",
        "\n",
        "a0 = torch.rand((4, 5), requires_grad=True)     #For the hidden layer\n",
        "b0 = torch.rand((1, 5), requires_grad=True)\n",
        "\n",
        "a1 = torch.rand((5,2 ), requires_grad=True)     #For the output layer.    Fill in the dimensions appropriately\n",
        "b1 = torch.rand((1,2 ), requires_grad=True)"
      ],
      "metadata": {
        "id": "wErkLlPkeTUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnVdvfk-Zsyr",
        "outputId": "205165e9-9a65-4d25-bf95-65ecb55eae9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5923408269882202\n",
            "1.5661039352416992\n",
            "1.5400670766830444\n"
          ]
        }
      ],
      "source": [
        "#Assuming quantile to be 0.5\n",
        "\n",
        "# Forward Pass 1\n",
        "y_1 = x@a0 + b0\n",
        "y_pred1= y_1@a1 + b1\n",
        "\n",
        "def loss_function(output, target):\n",
        "  a = target - output\n",
        "  quantile = 0.5\n",
        "  loss = torch.max((quantile - 1) * (a), quantile * (-1*a))\n",
        "  loss = torch.abs(loss).mean()                                 #Fill the loss function here\n",
        "  return loss\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred1, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propogation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 = a0 - 0.01*a0.grad\n",
        "    b0 = b0 - 0.01*b0.grad\n",
        "    a1 = a1 - 0.01*a1.grad\n",
        "    b1 = b1 - 0.01*b1.grad\n",
        "\n",
        "a0.requires_grad = True\n",
        "a1.requires_grad = True\n",
        "b0.requires_grad = True\n",
        "b1.requires_grad = True\n",
        "\n",
        "# Forward Pass 2\n",
        "y_2 = x@a0 + b0\n",
        "y_pred2= y_2@a1 + b1                                      #Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred2, y)\n",
        "print(loss.item())\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 = a0 - 0.01*a0.grad\n",
        "    b0 = b0 - 0.01*b0.grad\n",
        "    a1 = a1 - 0.01*a1.grad\n",
        "    b1 = b1 - 0.01*b1.grad\n",
        "a0.requires_grad = True\n",
        "a1.requires_grad = True\n",
        "b0.requires_grad = True\n",
        "b1.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "# Forward Pass 3\n",
        "y_3 = x@a0 + b0\n",
        "y_pred3= y_3@a1 + b1\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred3, y)\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting a graph with loss(at the end) and quantile\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = torch.rand((2, 4), requires_grad=False)\n",
        "y = torch.rand((2, 2), requires_grad=False)\n",
        "\n",
        "a0 = torch.rand((4, 5), requires_grad=True)\n",
        "b0 = torch.rand((1, 5), requires_grad=True)\n",
        "\n",
        "a1 = torch.rand((5, 2), requires_grad=True)\n",
        "b1 = torch.rand((1, 2), requires_grad=True)\n",
        "\n",
        "quantile_list = [round(i * 0.05,2) for i in range(1, 20)]\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "def loss_function(output, target):\n",
        "    a = target - output\n",
        "    loss = torch.max((quantile - 1) * a, quantile * (-1*a))\n",
        "    loss = torch.abs(loss).mean()\n",
        "    return loss\n",
        "\n",
        "for quantile in quantile_list:\n",
        "\n",
        "\n",
        "  # Forward Pass\n",
        "  y_1 = x @ a0 + b0\n",
        "  y_pred1 = y_1 @ a1 + b1\n",
        "\n",
        "  # Computing Loss\n",
        "  loss = loss_function(y_pred1, y)\n",
        "\n",
        "  # Back Propogation\n",
        "  loss.backward()\n",
        "\n",
        "  # Updating Gradients\n",
        "  with torch.no_grad():\n",
        "    a0 = a0 - 0.01 * a0.grad\n",
        "    b0 = b0 - 0.01 * b0.grad\n",
        "    a1 = a1 - 0.01 * a1.grad\n",
        "    b1 = b1 - 0.01 * b1.grad\n",
        "\n",
        "  a0.requires_grad = True\n",
        "  a1.requires_grad = True\n",
        "  b0.requires_grad = True\n",
        "  b1.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "    # Forward Pass 2\n",
        "  y_2 = x@a0 + b0\n",
        "  y_pred2= y_2@a1 + b1                                      #Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "  loss = loss_function(y_pred2, y)\n",
        "\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "  with torch.no_grad():\n",
        "    a0 = a0 - 0.01*a0.grad\n",
        "    b0 = b0 - 0.01*b0.grad\n",
        "    a1 = a1 - 0.01*a1.grad\n",
        "    b1 = b1 - 0.01*b1.grad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Forward Pass 3\n",
        "  y_3 = x@a0 + b0\n",
        "  y_pred3= y_3@a1 + b1\n",
        "\n",
        "# Computing Loss\n",
        "  loss = loss_function(y_pred3, y)\n",
        "  loss_list.append(loss.item())\n",
        "# Plot the results\n",
        "plt.plot(quantile_list, loss_list)\n",
        "plt.xlabel(\"Quantile\")\n",
        "plt.ylabel(\"Final Loss\")\n",
        "plt.title(\"Variation of Quantile with Final Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mzhYJ9X6Z2IM",
        "outputId": "f076def5-2ed2-4186-b9a3-088c7c8c9154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8ffd6ccbafc8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Back Propogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# Updating Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izhl-9Wta_uu"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}